"""This file is a simple wrapper for encode_fair_esm, which is FAIR's
python one-step script for generating embeddings using their model."""

#Author: Jonathan Parkinson <jlparkinson1@gmail.com>

import os, sys, subprocess, torch


def fair_esm_wrapper(start_dir):
    """Wrapper for the encode_fair_esm. Check whether the training and test
    set have already been encoded and if not, encode as appropriate."""
    os.chdir(os.path.join(start_dir, "encoded_data"))
    if "fair_esm_trainx.pt" not in os.listdir() or "fair_esm_trainy.pt" \
            not in os.listdir():
        print("FAIR-ESM training set encoding not found "
        " -- will generate, this might take a while...")
        encode_fasta_file("train", start_dir)
        print("FAIR-ESM training set encoding generated.")
    else:
        print("FAIR-ESM training set found.")

    if "fair_esm_testx.pt" not in os.listdir() or "fair_esm_testy.pt" \
            not in os.listdir():
        print("FAIR-ESM test set encoding not found "
        " -- will generate, this might take a while...")
        encode_fasta_file("test", start_dir)
        print("FAIR-ESM test set encoding generated.")
                
    else:
        print("FAIR-ESM training set found.")


def encode_fasta_file(fasta_type, start_dir):
    """Encode a specific fasta file (either training or test). First we
    use subprocess to call FAIR's script to run their model (their model
    is expensive to run, so the user should be prepared for some wait time).
    Next we take the pt files generated by their script and put these together
    into a single file. The data necessary for the y-tensor is stored in the
    fasta file labels as for Unirep.

    Args:
        fasta_type (str): Either 'training' or 'test'.
        start_dir (str): A path to the start directory.
    """
    extract_loc = os.path.join(start_dir, "src", "sequence_encoding",
            "encode_fair_esm.py")
    if "%s_fair_esm"%fasta_type not in os.listdir():
        subprocess.call(["python", extract_loc, "esm1b_t33_650M_UR50S",
            "fair_unirep_%s.faa"%fasta_type, "%s_fair_esm"%fasta_type,
            "--include", "mean", "per_tok"])
    os.chdir("%s_fair_esm"%fasta_type)
    x, y = [], []
    print("Converting FAIR-ESM tensors to single file...one minute...")
    for i, ptfile in enumerate(os.listdir()):
        seqdata = torch.load(ptfile)
        x.append(seqdata["mean_representations"][33].unsqueeze(0))
        yrep = [float(z) for z in seqdata["label"].split("_")[1:]]
        yrep = torch.tensor(yrep).unsqueeze(0)
        y.append(yrep)
        if i % 250 == 0:
            print("%s complete"%i)

    y, x = torch.cat(y, dim=0), torch.cat(x, dim=0)
    os.chdir("..")
    torch.save(x, "fair_esm_%sx.pt"%fasta_type)
    torch.save(y, "fair_esm_%sy.pt"%fasta_type)

