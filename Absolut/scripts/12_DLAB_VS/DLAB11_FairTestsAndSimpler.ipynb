{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "#Script that performs DLAB-vs on Absolut! lattice pairs (of size 6x6)\n",
    "#Usage in command line:\n",
    "#python ThisFile.py nAntigens=100 nPairsTot=10000 startegyNegatives=1 condition=1 nRotations=20 groupSize=50 fnatBind=0.9 fnatDLABNegative=0.1 balancingStrategy=0 nRepeats=1 nEpochs=5 batch_size=2000\n",
    "\n",
    "#New things, now it includes 1 more strategy for negatives (separating Others(100) versus low fnat (1000) \n",
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "from collections import Counter\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "random.seed(a=None, version=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Meaning of command-line parameters\n",
    "# nAntigens = 100       - number of antigens we learn from [so we could use the others as transferrability]\n",
    "# nPairsTot = 10000     - total number of pairs BEFORE data enhancement by rotation\n",
    "# strategyNegatives=1   - 1 = DLAB negatives other pairs. add 10 = with low affinity. add 100 = with absolut non-binders. add 1000 = with low fnat of binding pairs \n",
    "#                         interesting ones: 1001 (DLAB), 1111(all), and 110 (no DLAB, only non binders), 1 (DLAB, only nonbinding pairs but no low fnat)\n",
    "# strategyNegativeTesting=1   - 1 = DLAB negatives other pairs. add 10 = with low affinity. add 100 = with absolut non-binders. add 1000 = with low fnat of binding pairs \n",
    "# condition=1           - 1 one-hot, 3, shuffled, 11 = Chemical, 13 = Chemical shuffled\n",
    "# nRotations            - 0 for no data enhancement\n",
    "# groupSize = 1         - when pick a pair, how many poses of the same pair we consider\n",
    "# fnatBind = 0.9\n",
    "# fnatDLABNegative=0.1\n",
    "# balancingStrategy = 0/1/2    # 0: unbalanced; 1:Includes weights during fitting; 2:Find a way to balance the inputs inside batches\n",
    "# nRepeats                - will repeat the SAME selection of antigens multiple times. Restart multiple times with nRepeats=1 to have different antigens each time\n",
    "# nEpochs\n",
    "# batch_size\n",
    "\n",
    "# Manually set options\n",
    "runningInCommandLine = False     # with False it runs some examples / tests\n",
    "LocationDataFiles = \"\"            # The preprocessed files are expected to be there\n",
    "size_lattice = 6\n",
    "stopDataLeakage = True           # Avoids data enhancement of the same pose to be in train and test\n",
    "doAllAntigens = False              # Allows to use data on all antigens (if False, only considers 1ADQ_A, for debugging)\n",
    "removeFirstDimention = True        # In one-hot encoding, removes the dimension meaning \"empty grid position\", True => 20 dims. False => 21 dims\n",
    "\n",
    "\n",
    "# Default parameters similar to DLAB\n",
    "nAntigens = 1\n",
    "nPairsTot = 5000            #this is before data augmentation => the training size will be 0.8 * nPairsTot * nRotations\n",
    "strategyNegatives = 1001       #only DLAB negatives, no other ones\n",
    "strategyNegativeTesting = 1110 # We will show two test: itself (test=strategyNegatives) and external (ext=strategyNegativeTesting)\n",
    "\t\t\t\t\t\t\t   # Further, we will also balance the external dataset to have the same amount of positives as the test\n",
    "condition = 1\n",
    "nRotations = 1 #20\n",
    "groupSize = 50                #this is already done during preprocessing, so we will just read preprocessed files with this value of groupSize\n",
    "fnatBind = 0.9\n",
    "fnatDLABNegative=0.1\n",
    "balancingStrategy = 0\n",
    "nRepeats = 1\n",
    "nEpochs = 5\n",
    "batch_size = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering command line arguments\n",
    "repeat = 1\n",
    "listAntigens = [\"1ADQ_A\"]\n",
    "if(doAllAntigens):\n",
    "    listAntigens = [\"1ADQ_A\", \"1FBI_X\", \"1FNS_A\", \"1FSK_A\", \"1H0D_C\", \"1JPS_T\", \"1KB5_AB\", \"1NCB_N\", \"1NSN_S\", \"1OAZ_A\", \"1OB1_C\", \"1OSP_O\", \"1PKQ_J\", \"1QFW_AB\", \"1RJL_C\", \"1S78_B\", \"1TQB_A\", \"1WEJ_F\", \"1YJD_C\", \"1ZTX_E\", \"2ARJ_RQ\", \"2B2X_A\", \"2FD6_AU\", \"2HFG_R\", \"2IH3_C\", \"2JEL_P\", \"2Q8A_A\", \"2R29_A\", \"2R4R_A\", \"2R56_A\", \"2UZI_R\", \"2VXQ_A\", \"2VXT_I\", \"2W9E_A\", \"2WUC_I\", \"2XQB_A\", \"2XWT_C\", \"2YC1_C\", \"2YPV_A\", \"2ZCH_P\", \"3BGF_S\", \"3BN9_A\", \"3CVH_ABC\", \"3DVG_XY\", \"3EFD_K\", \"3GI9_C\", \"3HI6_A\", \"3JBQ_B\", \"3KJ4_A\", \"3KR3_D\", \"3KS0_J\", \"3L5X_A\", \"3L95_X\", \"3MJ9_A\", \"3NCY_A\", \"3NFP_I\", \"3NH7_A\", \"3Q3G_E\", \"3R08_E\", \"3R1G_B\", \"3RAJ_A\", \"3RKD_A\", \"3RVV_A\", \"3SKJ_E\", \"3SQO_A\", \"3TT1_A\", \"3U9P_C\", \"3UBX_A\", \"3V6O_A\", \"3VG9_A\", \"3VRL_C\", \"3WD5_A\", \"4AEI_A\", \"4CAD_C\", \"4DKE_A\", \"4H88_A\", \"4HC1_B\", \"4HJ0_B\", \"4I18_R\", \"4I77_Z\", \"4K24_A\", \"4K3J_A\", \"4K9E_C\", \"4KI5_M\", \"4KXZ_A\", \"4LU5_B\", \"4MXV_B\", \"4N9G_C\", \"4NP4_A\", \"4OII_A\", \"4OKV_E\", \"4PP1_A\", \"4QCI_D\", \"4QEX_A\", \"4QNP_A\", \"4QWW_A\", \"4R9Y_D\", \"4RGM_S\", \"4U1G_A\", \"4U6V_A\", \"4WV1_F\", \"4Y5V_C\", \"4YPG_C\", \"4YUE_C\", \"4ZFG_A\", \"4ZFO_F\", \"4ZSO_E\", \"5B8C_C\", \"5BVP_I\", \"5C0N_A\", \"5C7X_A\", \"5CZV_A\", \"5D93_A\", \"5DFV_A\", \"5DHV_M\", \"5DMI_A\", \"5DO2_B\", \"5E8D_A\", \"5E8E_LH\", \"5E94_G\", \"5EII_G\", \"5EPM_C\", \"5EU7_A\", \"5EZO_A\", \"5F3B_C\", \"5FB8_C\", \"5H35_C\", \"5HDQ_A\", \"5HI4_B\", \"5IKC_M\", \"5J13_A\", \"5JW4_A\", \"5JZ7_A\", \"5KN5_C\", \"5KTE_A\", \"5L0Q_A\", \"5LQB_A\", \"5MES_A\", \"5T5F_A\", \"5TH9_A\", \"5TLJ_X\", \"5TZ2_C\"]\n",
    "    listAntigens = [\"1ADQ_A\", \"1FBI_X\", \"1FNS_A\", \"1FSK_A\", \"1H0D_C\", \"1JPS_T\", \"1KB5_AB\", \"1NCB_N\", \"1NSN_S\", \"1OAZ_A\", \"1OB1_C\", \"1OSP_O\", \"1PKQ_J\", \"1QFW_AB\", \"1RJL_C\", \"1TQB_A\", \"1WEJ_F\", \"1YJD_C\", \"1ZTX_E\", \"2ARJ_RQ\", \"2B2X_A\", \"2FD6_AU\", \"2HFG_R\", \"2IH3_C\", \"2JEL_P\", \"2Q8A_A\", \"2R29_A\", \"2R4R_A\", \"2R56_A\", \"2UZI_R\", \"2VXQ_A\", \"2VXT_I\", \"2W9E_A\", \"2WUC_I\", \"2XQB_A\", \"2XWT_C\", \"2YC1_C\", \"2YPV_A\", \"2ZCH_P\", \"3BGF_S\", \"3BN9_A\", \"3DVG_XY\", \"3EFD_K\", \"3GI9_C\", \"3HI6_A\", \"3JBQ_B\", \"3KJ4_A\", \"3KR3_D\", \"3KS0_J\", \"3L5X_A\", \"3L95_X\", \"3MJ9_A\", \"3NFP_I\", \"3NH7_A\", \"3Q3G_E\", \"4R9Y_D\", \"4RGM_S\", \"4U1G_A\", \"4U6V_A\", \"4WV1_F\", \"4Y5V_C\", \"4YPG_C\", \"4YUE_C\", \"4ZFG_A\", \"4ZFO_F\", \"4ZSO_E\", \"5B8C_C\", \"5BVP_I\", \"5C0N_A\", \"5C7X_A\", \"5CZV_A\", \"5D93_A\", \"5DFV_A\", \"5DHV_M\", \"5DMI_A\", \"5DO2_B\", \"5E8D_A\", \"5E8E_LH\", \"5E94_G\", \"5JW4_A\"]\n",
    "    listAntigens = [\"1ADQ_A\", \"1FBI_X\", \"1FNS_A\", \"1FSK_A\", \"1H0D_C\", \"1JPS_T\", \"1KB5_AB\", \"1NCB_N\", \"1NSN_S\", \"1OAZ_A\", \"1OB1_C\"]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not(runningInCommandLine)):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "if(runningInCommandLine):\n",
    "    if(len(sys.argv) > 1):\n",
    "        nAntigens = int(sys.argv[1])\n",
    "        if(doAllAntigens == False and nAntigens > 1):\n",
    "            print(\"ERR: you are in debug mode (doAllAntigens == False), only one antigen allowed (will be 1ADQ_A)\")\n",
    "            sys.exit()\n",
    "\n",
    "    if(len(sys.argv) > 2):\n",
    "        nPairsTot = int(sys.argv[2])\n",
    "\n",
    "    if(len(sys.argv) > 3):\n",
    "        strategyNegatives = int(sys.argv[3])\n",
    "\n",
    "    if(len(sys.argv) > 4):\n",
    "        strategyNegativeTesting = int(sys.argv[4])\n",
    "\n",
    "    if(len(sys.argv) > 5):\n",
    "        condition = int(sys.argv[5])\n",
    "\n",
    "    if(len(sys.argv) > 6):\n",
    "        nRotations = int(sys.argv[6])\n",
    "\n",
    "    if(len(sys.argv) > 7):\n",
    "        groupSize = int(sys.argv[7])\n",
    "        \n",
    "    if(len(sys.argv) > 8):\n",
    "        fnatBind = float(sys.argv[8])\n",
    "\n",
    "    if(len(sys.argv) > 9):\n",
    "        fnatDLABNegative = float(sys.argv[9])\n",
    "\n",
    "    if(len(sys.argv) > 10):\n",
    "        balancingStrategy = int(sys.argv[10])\n",
    "\n",
    "    if(len(sys.argv) > 11):\n",
    "        nRepeats = int(sys.argv[11])\n",
    "\n",
    "    if(len(sys.argv) > 12):\n",
    "        nEpochs = int(sys.argv[12])\n",
    "\n",
    "    if(len(sys.argv) > 13):\n",
    "        batch_size = int(sys.argv[13])\n",
    "\n",
    "#sanity checks\n",
    "if(nRepeats > 10):\n",
    "    print(\"ERR: We don't allow more than 10 repeats because we have only preprocessed 10 different files per antigen. Comment this line if you have preprocessed more\")\n",
    "    sys.exit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveStrategyNegatives = strategyNegatives\n",
    "\n",
    "#translating strategyNegatives into booleans\n",
    "includeLowFnatAsNeg = False\n",
    "if(strategyNegatives >= 1000):\n",
    "    includeLowFnatAsNeg = True\n",
    "    strategyNegatives = strategyNegatives - 1000\n",
    "\n",
    "includeNonBinders = False\n",
    "if(strategyNegatives >= 100):\n",
    "    includeNonBinders = True\n",
    "    strategyNegatives = strategyNegatives - 100\n",
    "\n",
    "includeLowAff = False\n",
    "if(strategyNegatives >= 10):\n",
    "    includeLowAff = True\n",
    "    strategyNegatives = strategyNegatives - 10\n",
    "    \n",
    "includeDLABneg = False\n",
    "if(strategyNegatives == 1):\n",
    "    includeDLABneg = True\n",
    "\n",
    "\n",
    "saveStrategyNegativesTesting = strategyNegativeTesting\n",
    "includeLowFnatAsNegExt = False\n",
    "if(strategyNegativeTesting >= 1000):\n",
    "    includeLowFnatAsNegExt = True\n",
    "    strategyNegativeTesting = strategyNegativeTesting - 1000\n",
    "\n",
    "includeNonBindersExt = False\n",
    "if(strategyNegativeTesting >= 100):\n",
    "    includeNonBindersExt = True\n",
    "    strategyNegativeTesting = strategyNegativeTesting - 100\n",
    "\n",
    "includeLowAffExt = False\n",
    "if(strategyNegativeTesting >= 10):\n",
    "    includeLowAffExt = True\n",
    "    strategyNegativeTesting = strategyNegativeTesting - 10\n",
    "    \n",
    "includeDLABnegExt = False\n",
    "if(strategyNegativeTesting == 1):\n",
    "    includeDLABnegExt = True\n",
    "\n",
    "\n",
    "\n",
    "saveCondition = condition\n",
    "useAAchem = False\n",
    "if(condition > 10):\n",
    "    useAAchem = True\n",
    "    condition = condition - 10\n",
    "\n",
    "if removeFirstDimention:\n",
    "    encodingDimension = 20\n",
    "else:\n",
    "    encodingDimension = 21\n",
    "    \n",
    "if useAAchem == True:\n",
    "    encodingDimension = encodingDimension - 16\n",
    "    print(\"ERR: the chemical encoding (4 characters, c n p r) is not yet implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= 1: scripts for reading data preprocessing , performed in advance ===========\n",
    "# Since the raw poses take too much space (4 TB), it is not possible to regenerate a train/test dataset from them anymore.\n",
    "# This is done by a separate script, that takes reads raw poses and returns only a number of poses filtered by the options:\n",
    "#                                     Antigen nPosesPerLabelType   thresFnatBind  thresFnatNegative  PosesPerCDRH3\n",
    "# example: python PreprocessScript.py 4R9Y_D  10000                0.9            0.1                50\n",
    "# Of note, this script selects each label type (P, L, N, O, I, F but not ?), with 10 000 of each label. \n",
    "# Therefore, the antigen is processed once, and one can later filter which types of labels one consider.\n",
    "# However, for different thresholds, one need to geenrate new files\n",
    "# Raw input file expected:\n",
    "# \"Poses_\" + antigenID + \"_PosesSourceSlices\" + antigenID + \".txt_1000_6.txt\")\n",
    "# Output filtered files (10 of them)\n",
    "# \"PrePro\" + antigenID + \"_\" + str(nPairsTot) + \"_\" + str(fnatBind) + \"_\" + str(fnatDLABNegative) + \"_\" + str(groupSize) + \"v\" + str(repeat) + \".txt\"\n",
    "\n",
    "# For info, meaning of labels that are already annotated to the sequences\n",
    "# L: Low affinity => includeLowAff\n",
    "# N: Non binders => includeNonBinders\n",
    "# O: binding to other ones\n",
    "# F: FALSE negatives (binding to other ones but actually binding this one as well) \n",
    "# P: Positive binding pose (always taken)\n",
    "# I: Incorrect pose (negative < fnat limit) => includeDLABneg\n",
    "# ?: Pose that is not taken (neither positive nor negative)\n",
    "#    O + F + I are the \"DLAB negatives\". They don't know a sequence is F, in DLAB it belongs to O\n",
    "\n",
    "def preProFileToOpen(antigenID = \"1ADQ_A\", nPairsTot = 10000, repeat = 1): #PrePro1ADQ_A_10000_0.9_0.1_50v1\n",
    "    # We have generated the files for 10 000 poses and less. For more, the exact file should be around\n",
    "    if(nPairsTot < 10000):\n",
    "        nPairsTot = 10000 #remember this is a local variable\n",
    "    return(LocationDataFiles + \"PrePro\" + antigenID + \"_\" + str(nPairsTot) + \"_\" + str(fnatBind) + \"_\" + str(fnatDLABNegative) + \"_\" + str(groupSize) + \"v\" + str(repeat) + \".txt\")\n",
    "\n",
    "#repeat from 1 to 9\n",
    "def openPreprocessedPosesOneAntigen(antigenID, repeat=1, includeNonBinders = True, includeLowAff = True, includeDLABneg = True, \n",
    "                        includeLowFnatAsNeg = True, thresholdFnatBinder = 0.9, thresholdFnatNegative = 0.1):\n",
    "    \n",
    "    print(\"Opening\", preProFileToOpen(antigenID, repeat))\n",
    "    prepro = open(preProFileToOpen(antigenID, repeat), newline = '')   #one line is a text with \\t and \\n                                                                              \n",
    "    data_reader = csv.reader(prepro, delimiter='\\t') #transform lines into lists \n",
    "    #    IDs    antigenLattice    antibodyLattice    labels    fnats    gid\n",
    "    # plus le numero de ligne, damned\n",
    "    \n",
    "    IDs = []\n",
    "    antigenLattice = []\n",
    "    antibodyLattice = []\n",
    "    labels = [] # P = Positive /L = Low affinity /N = Nonbinder /O = Other AGs (DLAB negative) - only P is positive.\n",
    "    fnats = []  # FNAT score, or -1 if non-binder.\n",
    "    \n",
    "    for line in data_reader:\n",
    "        \n",
    "        takeThisLine = True\n",
    "        \n",
    "        if(data_reader.line_num == 1):\n",
    "            takeThisLine = False #headers\n",
    "        \n",
    "        currentLabel = line[4]\n",
    "        \n",
    "        \n",
    "        if(currentLabel == 'N' and includeNonBinders == False):\n",
    "            takeThisLine = False\n",
    "        if(currentLabel == 'L' and includeLowAff == False):\n",
    "            takeThisLine = False\n",
    "        if(currentLabel == 'I' and includeLowFnatAsNeg == False):\n",
    "            takeThisLine = False\n",
    "        if(currentLabel == 'F' and includeDLABneg == False):\n",
    "            takeThisLine = False\n",
    "        if(currentLabel == 'O' and includeDLABneg == False):\n",
    "            takeThisLine = False\n",
    "\n",
    "        if(takeThisLine):\n",
    "            IDs.append(line[1])\n",
    "            antigenLattice.append(line[2])\n",
    "            antibodyLattice.append(line[3])\n",
    "            labels.append(currentLabel)\n",
    "            fnats.append(line[5])\n",
    "        \n",
    "    return([IDs, antigenLattice, antibodyLattice, labels, fnats])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] (4, 20)\n",
      "A\n",
      "A_CD\n",
      "[['______', '______', '______', '______', '______', '______'], ['______', 'H_____', '______', '______', '______', '______'], ['______', '______', '__W___', '__Y___', '______', '______'], ['______', '______', '__L___', '__D___', '______', '______'], ['______', '______', '______', '______', '______', '______'], ['______', '______', '______', '______', '______', '______']]\n",
      "None ()\n",
      "[[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]] (6, 6, 20)\n",
      "(6, 6, 6, 20)\n"
     ]
    }
   ],
   "source": [
    "# ========= 2: scripts for one-hot encoding of lattices ===========    \n",
    "    \n",
    "# The first dimension will represent '_', that is an empty position. We will encode it as zero.\n",
    "# we will then remove this dimension because dummy (if all other ones are zero, this one is 1), so it remains only the 20 AAs\n",
    "# see removeFirstDimention = True in manually set options\n",
    "\n",
    "#Defining the one hot encoding\n",
    "alphabet = '_ACDEFGHIKLMNPQRSTVWY'\n",
    "\n",
    "if useAAchem:\n",
    "    alphabet = '_cpnr'\n",
    "\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "chemCode = {\n",
    "    'D': 'c',\n",
    "    'E': 'c',\n",
    "    'H': 'c',\n",
    "    'R': 'c',\n",
    "    'K': 'c',\n",
    "    'S': 'p',\n",
    "    'T': 'p',\n",
    "    'N': 'p',\n",
    "    'Q': 'p',\n",
    "    'G': 'n',\n",
    "    'A': 'n',\n",
    "    'V': 'n',\n",
    "    'L': 'n',\n",
    "    'I': 'n',\n",
    "    'P': 'n',\n",
    "    'M': 'n',\n",
    "    'C': 'n',\n",
    "    'F': 'r',\n",
    "    'W': 'r',\n",
    "    'Y': 'r',\n",
    "    'B': '!',\n",
    "    'J': '!',\n",
    "    'O': '!',\n",
    "    'U': '!',\n",
    "    'X': '!',\n",
    "    'Z': '!',\n",
    "    '_': '_'\n",
    "}\n",
    "\n",
    "def stringToAAChemProp(s):\n",
    "    return \"\".join(chemCode[l] for l in s)\n",
    "\n",
    "def hotEncodingAAString(myString):\n",
    "    onehot_encoded = list()\n",
    "    integer_encoded = [char_to_int[char] for char in myString]\n",
    "    for value in integer_encoded:\n",
    "        letter = [0 for _ in range(len(alphabet))]\n",
    "        letter[value] = 1        \n",
    "        if(removeFirstDimention == True):\n",
    "            letter = letter[1:len(alphabet)]\n",
    "        \n",
    "        onehot_encoded.append(letter)\n",
    "\n",
    "    return onehot_encoded  #Note, in previous versions I was returning [], which has shape [1, N, Vocab], here without 1\n",
    "\n",
    "#input: [0,0,...,1,0,0]\n",
    "def retrieveAA(onehot_encodedAA):\n",
    "    foundAA = '_'\n",
    "    for i in range(len(onehot_encodedAA)):\n",
    "        if(onehot_encodedAA[i] == 1):\n",
    "            if(removeFirstDimention == True):\n",
    "                foundAA = int_to_char[i+1]\n",
    "            else:\n",
    "                foundAA = int_to_char[i]\n",
    "    return foundAA\n",
    "\n",
    "#input: [[0,0,...,1,0,0] , ... , [0,1,0,0... 0]]\n",
    "def retrieveString(onehot_encodedString):\n",
    "    foundString = \"\"\n",
    "    for k in range(len(onehot_encodedString)):\n",
    "        foundString = foundString + retrieveAA(onehot_encodedString[k])\n",
    "    return foundString\n",
    "\n",
    "# Converts the text description of a lattice into a 2D list of string blocks, that can be directly one-hot encoded. \n",
    "def textTo2Dtext(lattice):\n",
    "    lineX = lattice.split(',')[0:-1]    #the last character is a coma, so there is an empty block after\n",
    "    res = [X.strip().split(' ') for X in lineX]\n",
    "    print(res)\n",
    "    \n",
    "# one-hot a 1D of strings\n",
    "# HERE we are going to change into chemical based on the global variable useAAchem\n",
    "def batchOneHot(vecOfStrings):\n",
    "    if useAAchem:\n",
    "        return [hotEncodingAAString(stringToAAChemProp(s)) for s in vecOfStrings]\n",
    "    else:\n",
    "        return [hotEncodingAAString(s) for s in vecOfStrings]\n",
    "\n",
    "# one-hot a 2D of strings\n",
    "def textToTensor(lattice):\n",
    "    lineX = lattice.split(',')[0:-1]  #the last character is a coma, so there is an empty block after\n",
    "    return [batchOneHot(X.strip().split(' ')) for X in lineX]\n",
    "\n",
    "#examples\n",
    "if(not(runningInCommandLine)):\n",
    "    test = hotEncodingAAString(\"A_CD\")\n",
    "    print(test, np.array(test).shape)\n",
    "    \n",
    "    found = retrieveAA(test[0])\n",
    "    print(found)\n",
    "    \n",
    "    got = retrieveString(test)\n",
    "    print(got)\n",
    "    \n",
    "    example = \"______ ______ ______ ______ ______ ______ ,______ H_____ ______ ______ ______ ______ ,______ ______ __W___ __Y___ ______ ______ ,______ ______ __L___ __D___ ______ ______ ,______ ______ ______ ______ ______ ______ ,______ ______ ______ ______ ______ ______ ,\"\n",
    "    v1 = textTo2Dtext(example)\n",
    "    print(v1, np.array(v1).shape)\n",
    "    \n",
    "    v2 = batchOneHot(['__P___', '______', '______', '______', '______', '______'])\n",
    "    print(v2, np.array(v2).shape)\n",
    "    \n",
    "    v3 = textToTensor(example)\n",
    "    print(np.array(v3).shape)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated  551342 possible uniformly distributed rotations\n",
      "1 0\n",
      "6 20\n",
      "11 65\n",
      "16 140\n",
      "21 241\n",
      "26 375\n",
      "31 529\n",
      "36 714\n",
      "41 926\n",
      "46 1168\n",
      "[-1.19069113106414, 0.14137167390749994]\n"
     ]
    }
   ],
   "source": [
    "# ========= 3 Functions for data enhancement by taking a 6x6x6x, lattice and rotating \"randomly\". ==========\n",
    "PI = 3.1415927535\n",
    "\n",
    "# This function returns a list of [Phi, theta] where phi goes -Pi to Pi and \n",
    "# theta goes -Pi/2 to Pi/2 and starts from the XY plane (the Z axis is theta = pi/2)\n",
    "def getDirections(resolution):\n",
    "    res = []\n",
    "    if (resolution < 1 ):\n",
    "        return(res)\n",
    "\n",
    "    # choice of latitude\n",
    "    for n in range(1, resolution):\n",
    "        thetaJ = PI * n / resolution - PI / 2.0 ;\n",
    "    \n",
    "        #choice of points\n",
    "        nj = math.floor( 0.5 + math.sqrt( 3 ) * resolution * math.cos(thetaJ))\n",
    "\n",
    "        shift = 0\n",
    "        if (n % 2 ) != 0 : \n",
    "            shift += 0.5 ;\n",
    "            for j in range(0, nj):\n",
    "        \n",
    "                # their phi is from equator (X,Y plane)\n",
    "                res.append([-PI + 2.0 * PI * ( j + shift) / nj, thetaJ])\n",
    "        \n",
    "    return res;\n",
    "    \n",
    "# Generates a pool of uniform directions that we are going as a library to pick randomly\n",
    "possibleRotations = getDirections(1000)\n",
    "random.shuffle(possibleRotations)\n",
    "totalRotations = len(possibleRotations)\n",
    "print(\"Generated \", totalRotations, \"possible uniformly distributed rotations\")\n",
    "\n",
    "def getRandomRotation():\n",
    "    return possibleRotations[random.randint(0, totalRotations-1)]\n",
    "\n",
    "# Example\n",
    "# To get the idea of how many uniformly distributed directions we have depending on the resolution\n",
    "if(not(runningInCommandLine)):\n",
    "    for i in range(1, 51, 5):\n",
    "        print(i, len(getDirections(i)))        \n",
    "    print(getRandomRotation())\n",
    "\n",
    "# Rotation function for [6x6x6] lattices\n",
    "# First centers to (2.5, 2.5, 2.5) then rotates according to angles phi and theta, then translate backs \n",
    "# Theta is the angle that preserves the Y axis (latitude, -90 to 90), Phi preserves the Z azis (longitude, 0 to 360).\n",
    "     # if rotates by theta = 90 deg (pi / 4) then X1 = -Z and Z1 = X, \n",
    "     # therefore, if the vector was (1, 0, 0) it becomes (0, 0, 1): Theta starts from flat (X,Y plane) towards Z:\n",
    "\n",
    "def rotate(pos3D, theta, phi):\n",
    "    x = pos3D[0] - 2.5;\n",
    "    y = pos3D[1] - 2.5;\n",
    "    z = pos3D[2] - 2.5;\n",
    "\n",
    "    X1 =  x*math.cos(theta)-z*math.sin(theta);\n",
    "    Y1 =  y;\n",
    "    Z1 =  x*math.sin(theta)+z*math.cos(theta);\n",
    "\n",
    "    X2 =  X1*math.cos(phi)+Y1*math.sin(phi);\n",
    "    Y2 =  -X1*math.sin(phi)+Y1*math.cos(phi);\n",
    "    Z2 =  Z1;\n",
    "\n",
    "    return([X2 + 2.5, Y2 + 2.5, Z2 + 2.5])\n",
    "\n",
    "# If a rotated position happens to be outside the lattice we bring it back in [we could also have decided to kick it out]\n",
    "def boxize6(floatNr):\n",
    "    rounded = round(floatNr)\n",
    "    if(rounded < 0):\n",
    "        return(0)\n",
    "    if(rounded > 5):\n",
    "        return(5)\n",
    "    return(rounded)\n",
    "\n",
    "# Of note, depending on removal of the dummy dimension '_' and on the chemical or AA encoding, the last dimension is 4, 5, 20 or 21\n",
    "checkdimEncoding = len(alphabet)\n",
    "startingRelevantDimension = 1\n",
    "if(removeFirstDimention == True):\n",
    "    checkdimEncoding = checkdimEncoding - 1\n",
    "    startingRelevantDimension = 0\n",
    "\n",
    "if checkdimEncoding != encodingDimension:\n",
    "    print(\"ERR: the alphabet and the encoding dimension were different.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Main function to rotate a lattice \n",
    "def rotateLattice(latticeTensor4D, phi_theta = None):\n",
    "    if(phi_theta == None):\n",
    "        [phi, theta] = getRandomRotation()\n",
    "    else:\n",
    "        [phi, theta] = phi_theta\n",
    "    \n",
    "    res = np.zeros(shape = [6,6,6,encodingDimension])\n",
    "    \n",
    "    if(not removeFirstDimention):\n",
    "        for i in range(0,6):\n",
    "            for j in range(0,6):\n",
    "                for k in range(0,6):\n",
    "                    res[i][j][k][0] = 1 # by default\n",
    "                \n",
    "    for i in range(0,6):\n",
    "        for j in range(0,6):\n",
    "            for k in range(0,6):\n",
    "                if(sum(latticeTensor4D[i][j][k][startingRelevantDimension:encodingDimension]) > 0):\n",
    "                    [newX, newY, newZ] = rotate([i,j,k], theta, phi)\n",
    "                    #print(i, j, k, \" -> \", newX, newY, newZ)\n",
    "                    res[boxize6(newX)][boxize6(newY)][boxize6(newZ)] = latticeTensor4D[i][j][k]\n",
    "    return(res)\n",
    "\n",
    "\n",
    "if(not(runningInCommandLine)):\n",
    "    example = \"______ ______ ______ ______ ______ ______ ,______ H_____ ______ ______ ______ ______ ,______ ______ __W___ __Y___ ______ ______ ,______ ______ __L___ __D___ ______ ______ ,______ ______ ______ ______ ______ ______ ,______ ______ ______ ______ ______ ______ ,\"\n",
    "    v = textToTensor(example)\n",
    "    \n",
    "    # Example of rotation where X becomes Z and Y stays the same.\n",
    "    rotateLattice(v, [0, PI/2.])\n",
    "    \n",
    "    res = rotateLattice(v)\n",
    "    #print(sum(res))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 4 - Definition of the DLAB architecture ================\n",
    "\n",
    "class CNN3D(tf.keras.Model):\n",
    "    def __init__(self, drop_rate = 0.2):\n",
    "        super(CNN3D, self).__init__()\n",
    "        \n",
    "        #self.mp1 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), strides=(2,2,2))\n",
    "                \n",
    "        #initializer1 = tf.keras.initializers.GlorotUniform()\n",
    "        self.conv1 = tf.keras.layers.Conv3D(\n",
    "            filters = 32,\n",
    "            kernel_size = 3,\n",
    "            padding = 'same',\n",
    "            strides = (1,1,1),\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        #self.mp2 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), strides=(2,2,2))\n",
    "\n",
    "        #initializer2 = tf.keras.initializers.GlorotUniform()\n",
    "        self.conv2 = tf.keras.layers.Conv3D(\n",
    "            #in_channels=32, \n",
    "            filters = 64, \n",
    "            kernel_size=3, \n",
    "            padding='valid',\n",
    "            strides = (1,1,1),\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        #self.mp3 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), strides=(2,2,2))\n",
    "        \n",
    "        #initializer3 = tf.keras.initializers.GlorotUniform()\n",
    "        self.conv3 = tf.keras.layers.Conv3D(\n",
    "            #in_channels=32, \n",
    "            filters = 128, \n",
    "            kernel_size=3, \n",
    "            padding='valid',\n",
    "            strides = (1,1,1),\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            activation='relu'\n",
    "        )        \n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "\n",
    "        # self.flat = tf.keras.layers.Flatten()\n",
    "        \n",
    "        #self.dropout = tf.keras.layers.Dropout(drop_rate)\n",
    "        \n",
    "        #self.fc1 = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform')\n",
    "\n",
    "\n",
    "        print(\"Initialized CNN3D\")\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        #x = self.mp1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        #x = self.mp2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        #x = self.mp3(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        # flatten\n",
    "        #x = self.flat(x)\n",
    "         # x = x.view(-1, self.num_flat_features(x))\n",
    "\n",
    "        #x = self.dropout(x)\n",
    "        #x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def createDLABmodel():\n",
    "    inps1 = tf.keras.layers.Input(shape=(size_lattice, size_lattice, size_lattice, encodingDimension), name = \"antibody\")\n",
    "    inps2 = tf.keras.layers.Input(shape=(size_lattice, size_lattice, size_lattice, encodingDimension), name = \"antigen\")\n",
    "\n",
    "    CNN1 = CNN3D()\n",
    "    features1 = CNN1(inps1)\n",
    "\n",
    "    CNN2 = CNN3D()\n",
    "    features2 = CNN1(inps2)\n",
    "\n",
    "    flat1 = tf.keras.layers.Flatten()\n",
    "    flat2 = tf.keras.layers.Flatten()\n",
    "\n",
    "    concat = tf.keras.layers.Concatenate() #axis=1)\n",
    "\n",
    "    dropout = tf.keras.layers.Dropout(0.2)\n",
    "\n",
    "    fc1 = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform')\n",
    "\n",
    "    flatenned1 = flat1(features1)\n",
    "    flatenned2 = flat2(features2)\n",
    "    output = concat([flatenned1, flatenned2])\n",
    "\n",
    "    output = dropout(output)\n",
    "    output = fc1(output)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[inps1, inps2], outputs=output)\n",
    "    return(model)    \n",
    "\n",
    "# tool function to shuffle something and return it    \n",
    "def internalShuffle(l):\n",
    "    random.shuffle(l)\n",
    "    return l\n",
    "\n",
    "def f1(precision, recall):\n",
    "    if((precision == 0) or (recall == 0)):\n",
    "        return(0)\n",
    "    \n",
    "    return((2 * precision * recall) / (precision + recall))\n",
    "# Functions defined!    \n",
    "    \n",
    "def printMetrics(evaluation, nEpochs):\n",
    "    f1_train = f1(evaluation['Precision'][nEpochs-1], evaluation['Recall'][nEpochs-1])\n",
    "    f1_val = f1(evaluation['val_Precision'][nEpochs-1], evaluation['val_Recall'][nEpochs-1])\n",
    "    res = [evaluation['loss'][nEpochs-1],  # : [0.07811099290847778, 0.021633705124258995],\n",
    "    evaluation['accuracy'][nEpochs-1],  #: [0.9834374785423279, 0.9943749904632568],\n",
    "    evaluation['FalseNegatives'][nEpochs-1],  #: [120.0, 45.0],\n",
    "    evaluation['FalsePositives'][nEpochs-1],  #: [145.0, 45.0],\n",
    "    evaluation['TrueNegatives'][nEpochs-1],  #: [14315.0, 14415.0],\n",
    "    evaluation['TruePositives'][nEpochs-1],  #: [1420.0, 1495.0],\n",
    "    evaluation['Precision'][nEpochs-1],  #: [0.9073482155799866, 0.9707792401313782],\n",
    "    evaluation['Recall'][nEpochs-1],  #: [0.9220778942108154, 0.9707792401313782],\n",
    "    evaluation['AUC'][nEpochs-1],\n",
    "    f1_train,\n",
    "    evaluation['val_loss'][nEpochs-1],  #: [10.108085632324219, 6.631629943847656],\n",
    "    evaluation['val_accuracy'][nEpochs-1],  #: [0.7537500262260437, 0.7742499709129333],\n",
    "    evaluation['val_FalseNegatives'][nEpochs-1],  #: [983.0, 878.0],\n",
    "    evaluation['val_FalsePositives'][nEpochs-1],  #: [2.0, 25.0],\n",
    "    evaluation['val_TrueNegatives'][nEpochs-1],  #: [2538.0, 2515.0],\n",
    "    evaluation['val_TruePositives'][nEpochs-1],  #: [477.0, 582.0],\n",
    "    evaluation['val_Precision'][nEpochs-1],  #: [0.9958246350288391, 0.9588138461112976],\n",
    "    evaluation['val_Recall'][nEpochs-1],  #: [0.32671234011650085, 0.39863014221191406],\n",
    "    evaluation['val_AUC'][nEpochs-1],\n",
    "    f1_val]\n",
    "\n",
    "    return(res)\n",
    "\n",
    "def getF1TrainVal(histDict):\n",
    "    L = len(histDict['accuracy'])\n",
    "    trainF1 = [f1(histDict['Precision'][i], histDict['Recall'][i]) for i in range(0, L)]\n",
    "    valF1 = [f1(histDict['val_Precision'][i], histDict['val_Recall'][i]) for i in range(0, L)]\n",
    "    return (trainF1, valF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection of  1 / 1 antigens: [0]\n",
      "Other antigens are kept as external dataset []\n",
      "Selected antigens:  ['1ADQ_A']\n",
      "AllOptions\tstopDataLeakage\tsize_lattice\tbalancingStrategy\tnEpochs\tnAntigens\tnPairsTot\tstrategyNegatives\t...\tnRepeats\tcondition\tuseAAchem\tnRotations\tgroupSize\tfnatBind\tfnatDLABNegative\tselectedAGnames\n",
      "\n",
      "AllOptions\t True\t6\t0\t5\t1\t5000\t1001\t...\t1\t1\tFalse\t1\t50\t0.9\t0.1\t['1ADQ_A']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========== 5 Now we read preprocessed files, make train / test and enhance by rotation ===========\n",
    "\n",
    "# selecting nAntigens randomly\n",
    "possibleIDs = [*range(0,len(listAntigens))]\n",
    "random.shuffle(possibleIDs)\n",
    "nAGsForTrain = min(len(listAntigens), nAntigens)\n",
    "\n",
    "selectedAGs = possibleIDs[0:nAGsForTrain]\n",
    "externalAGs = possibleIDs[nAGsForTrain:len(listAntigens)]\n",
    "print(\"Selection of \", nAntigens, \"/\", len(listAntigens) , \"antigens:\", selectedAGs)\n",
    "print(\"Other antigens are kept as external dataset\", externalAGs)\n",
    "selectedAGnames = [listAntigens[i] for i in selectedAGs]\n",
    "print(\"Selected antigens: \", selectedAGnames)\n",
    "\n",
    "\n",
    "# Summing up all options before we start\n",
    "print(\"AllOptions\\tstopDataLeakage\\tsize_lattice\\tbalancingStrategy\\tnEpochs\\tnAntigens\\tnPairsTot\\tstrategyNegatives\\t...\\tnRepeats\\tcondition\\tuseAAchem\\tnRotations\\tgroupSize\\tfnatBind\\tfnatDLABNegative\\tselectedAGnames\\n\")\n",
    "print(\"AllOptions\\t\", str(stopDataLeakage) + \"\\t\" + str(size_lattice) + \"\\t\" + str(balancingStrategy) + \"\\t\" + str(nEpochs) + \"\\t\" + str(nAntigens) + \"\\t\" + str(nPairsTot) + \"\\t\" + str(saveStrategyNegatives) + \"\\t\" + \"...\" + \"\\t\" + str(nRepeats)+ \"\\t\" + str(saveCondition) + \"\\t\" + str(useAAchem) + \"\\t\" + str(nRotations) + \"\\t\" + str(groupSize) + \"\\t\" + str(fnatBind) + \"\\t\" + str(fnatDLABNegative) + \"\\t\" + str(selectedAGnames) + \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeatLoop = 1\n",
    "#for repeatLoop in range(1, nRepeats+1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening PrePro1ADQ_A_10000_0.9_0.1_50v1.txt\n",
      "For antigen  1ADQ_A , got  500000  preprocessed antigen poses, strategy  1\n",
      "Counter({'O': 157350, 'F': 133400, 'L': 87850, 'N': 85850, 'P': 17850, 'I': 17700})\n",
      "Amount of each elements read: AGname, ID, antigenLattice, antibodyLattice, labels, fnats.\n",
      "500000 500000 500000 500000 500000 500000\n",
      "They should have the same value, it represents the number of different poses.\n",
      "Note: the poses are already per group of 50 pose for a single CDRH3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reading the preprocessed file of each antigen, \n",
    "pooledAGname = []\n",
    "pooledIDs = []\n",
    "pooledantigenLattice = []\n",
    "pooledantibodyLattice = []\n",
    "pooledlabels = []\n",
    "pooledfnats = []\n",
    "successedAntigens = []\n",
    "\n",
    "# When train and test have different composition of negatives, we will do as following:\n",
    "# - take all classes needed in either the train and/or test\n",
    "# - for the train, remove the non-wanted classes in train and separate into train and (pre-)test\n",
    "# - for the test, take the generated pre-test, remove the unwanted classes and add the possibly wanted\n",
    "#   that were not in the train.\n",
    "#   This will cause differences in the composition of the train and test. \n",
    "\n",
    "for ag in selectedAGnames:\n",
    "    try:\n",
    "        #f = open(preProFileToOpen(antigenID = \"1ADQ_A\", repeat = repeatLoop))\n",
    "        [IDs, antigenLattice, antibodyLattice, labels, fnats] = openPreprocessedPosesOneAntigen(ag, repeatLoop, includeNonBinders or includeNonBindersExt, includeLowAff or includeLowAffExt, includeDLABneg or includeDLABnegExt, includeLowFnatAsNeg or includeLowFnatAsNegExt, fnatBind, fnatDLABNegative)\n",
    "\n",
    "        print(\"For antigen \", ag, \", got \" , len(antigenLattice), \" preprocessed antigen poses, strategy \", strategyNegatives)\n",
    "        print(Counter(labels))\n",
    "\n",
    "        pooledAGname = pooledAGname + ([ag] * len(IDs))\n",
    "        pooledIDs = pooledIDs + IDs\n",
    "        pooledantigenLattice = pooledantigenLattice + antigenLattice\n",
    "        pooledantibodyLattice = pooledantibodyLattice + antibodyLattice\n",
    "        pooledlabels = pooledlabels + labels\n",
    "        pooledfnats = pooledfnats + fnats\n",
    "        successedAntigens = successedAntigens + [ag]\n",
    "    except IOError:\n",
    "        print(\"ERR: couldn't find preprocessed data for antigen \" + str(ag) + \", continuing\")\n",
    "\n",
    "print(\"Amount of each elements read: AGname, ID, antigenLattice, antibodyLattice, labels, fnats.\")\n",
    "print(len(pooledAGname), len(pooledIDs), len(pooledantigenLattice), len(pooledantibodyLattice), len(pooledlabels), len(pooledfnats))\n",
    "print(\"They should have the same value, it represents the number of different poses.\")\n",
    "print(\"Note: the poses are already per group of \" + str(groupSize) + \" pose for a single CDRH3\")\n",
    "\n",
    "# Now we will create 3 datasets: train (strategyNegatives, 80%), test (strategyNegatives, 20%), \n",
    "# \t\t\t\t\t\t\t\t and ext (test filtered for strategyNegativesExt + new negatives only in ext)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will add a new status to each instance, whether it belongs to train, train+ext or only ext\n",
    "def selectLabel(label, includeNonBinders, includeLowAff, includeDLABneg, includeLowFnatAsNeg):\n",
    "    if(includeNonBinders == True and label == 'N'): \n",
    "        return(1)\n",
    "    if(includeLowAff == True and label == 'L'): \n",
    "        return(1)\n",
    "    if(includeDLABneg == True and label == 'O'): \n",
    "        return(1)\n",
    "    if(includeLowFnatAsNeg == True and label == 'I'): \n",
    "        return(1)  \n",
    "    if(label == 'P'):\n",
    "        return(1)\n",
    "    return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example selectLabel('N', includeNonBinders, includeLowAff, includeDLABneg, includeLowFnatAsNeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooledIsInTrain = [selectLabel(label, includeNonBinders, includeLowAff, includeDLABneg, includeLowFnatAsNeg) \n",
    "                   for label in pooledlabels]\n",
    "pooledIsInExt = [selectLabel(label, includeNonBindersExt, includeLowAffExt, includeDLABnegExt, includeLowFnatAsNegExt) \n",
    "                   for label in pooledlabels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separatedAGname = pooledAGname + ([ag] * len(IDs))\n",
    "#    separatedIDs = pooledIDs + IDs\n",
    "#    separatedLattice = pooledantigenLattice + antigenLattice\n",
    "#    separatedLattice = pooledantibodyLattice + antibodyLattice\n",
    "#    separated = pooledlabels + labels\n",
    "#    separated = pooledfnats + fnats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffleListPositionsPerID(listIDs):\n",
    "    d = {}\n",
    "    for i in range(0, len(listIDs)):\n",
    "        if listIDs[i] in d.keys():\n",
    "            d[listIDs[i]].append(i)\n",
    "        else:\n",
    "            d[listIDs[i]] = [i]\n",
    "\n",
    "    # We will assume each key has the same amount of elements.\n",
    "    dicoPoseNames = Counter(listIDs)\n",
    "    possiblePosesIDs = list(dicoPoseNames.keys())\n",
    "    random.shuffle(possiblePosesIDs)\n",
    "\n",
    "    shuffledIDsgroupedPerPose = [ internalShuffle(d[i]) for i in possiblePosesIDs]\n",
    "    ResultPossibleDataIDs = [item for sublist in shuffledIDsgroupedPerPose for item in sublist]\n",
    "    return(ResultPossibleDataIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling the data is done independently of separarting the external data\n",
    "\n",
    "# Before generating rotations, if stopDataLeakage == True, we group poses from the same CDRH3 and shuffle them per group \n",
    "# it puts all generated rotations in a group that is going to be shuffled as a group\n",
    "# i.e. the rotations of the same pose stay together, so they don't end up in train AND test when cutting train = [indices 0... train size], etc\n",
    "\n",
    "# Makes dictionary: CDR3ID => list of IDs of the poses for it.\n",
    "if stopDataLeakage:\n",
    "    possibleDataIDs = shuffleListPositionsPerID(pooledIDs)\n",
    "    \n",
    "else:  # If false, We shuffle the rotations of the same pose, so they might be in train and test\n",
    "    possibleDataIDs = [*range(0,len(pooledIDs))]\n",
    "    random.shuffle(possibleDataIDs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that the possibleDataIDs are shuffled, we need to 'Mask' those not for the training\n",
    "# complex written: possibleDataIDs = [possibleDataIDs[i] for i in range(0,len(possibleDataIDs)) if pooledIsInTrain[possibleDataIDs[i]] == 1]\n",
    "possibleDataIDs = [pos for pos in possibleDataIDs if pooledIsInTrain[pos] == 1]\n",
    "allDataExt = [pos for pos in possibleDataIDs if pooledIsInExt[pos] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wished dataset sizes:\n",
      "Will keep only  5000  out of  192900 Available poses\n",
      "BEFORE data enhancement per rotation, expect  4000  training and  1000 validation poses\n"
     ]
    }
   ],
   "source": [
    "# selection of nPairsTot poses, and creation of nRotations copies of each pose. The first copy is not rotated.\n",
    "# Note, to make sure the same group of poses is not in train and test we will leave a gap of groupSize between them\n",
    "lenAvailable = len(possibleDataIDs)\n",
    "nPairsTot = min(nPairsTot,lenAvailable-groupSize)\n",
    "train_size = int(0.8 * nPairsTot)         # we already know how much will be the train and test\n",
    "test_size = int(0.2 * nPairsTot) \n",
    "print(\"Wished dataset sizes:\")\n",
    "print(\"Will keep only \", nPairsTot, \" out of \", lenAvailable, \"Available poses\")\n",
    "print(\"BEFORE data enhancement per rotation, expect \", train_size , \" training and \", test_size, \"validation poses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes in training: Counter({'O': 3150, 'P': 450, 'I': 400})\n",
      "Classes in validation: Counter({'O': 800, 'I': 150, 'P': 50})\n",
      "Labels in training: Counter({0: 3550, 1: 450})\n",
      "Labels in validation: Counter({0: 950, 1: 50})\n"
     ]
    }
   ],
   "source": [
    "# Converting all labels into 'P' => 1, all other ones => 0\n",
    "def binary(label):\n",
    "    if(label == 'P'):\n",
    "        return(1)\n",
    "\n",
    "    return(0)\n",
    "\n",
    "# Block for just for visualizing the data and stopping if only one class. (nothing from this block will be used)\n",
    "pooledlabelsAll = [pooledlabels[i] for i in possibleDataIDs[0:train_size + test_size + groupSize]]\n",
    "print(\"Classes in training:\", Counter(pooledlabelsAll[0:train_size]))\n",
    "print(\"Classes in validation:\", Counter(pooledlabelsAll[train_size + groupSize: train_size + test_size + groupSize]))\n",
    "encodedLabels = np.array(list(map(binary, pooledlabelsAll)))\n",
    "print(\"Labels in training:\", Counter(encodedLabels[0:train_size]))\n",
    "print(\"Labels in validation:\", Counter(encodedLabels[train_size + groupSize: train_size + test_size + groupSize]))\n",
    "if(len(np.unique(encodedLabels[0:train_size])) < 2):\n",
    "    print(\"ERR: The splitting of train and validation has raised only one class in the training\")\n",
    "    \n",
    "if(len(np.unique(encodedLabels[train_size + groupSize: train_size + test_size + groupSize])) < 2):\n",
    "    print(\"ERR: The splitting of train and validation has raised only one class in the validation\")\n",
    "\n",
    "# Now the \"external test\" will be: validation + (classes not in train) - (class in train but not in test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels Counter({0: 3550, 1: 450})\n",
      "Test labels Counter({0: 950, 1: 50})\n"
     ]
    }
   ],
   "source": [
    "# Now cutting the train and test according to block-shuffled data. We are still encoded as text, as in the input file\n",
    "PosElementsTrain = possibleDataIDs[0:train_size]\n",
    "PosElementsTest = possibleDataIDs[train_size + groupSize: train_size + test_size + groupSize]\n",
    "\n",
    "pooledAGnameShufTrain = [pooledAGname[i] for i in PosElementsTrain]\n",
    "pooledIDsShufTrain = [pooledIDs[i] for i in PosElementsTrain]\n",
    "pooledantigenLatticeShufTrain = [pooledantigenLattice[i] for i in PosElementsTrain]\n",
    "pooledantibodyLatticeShufTrain = [pooledantibodyLattice[i] for i in PosElementsTrain]\n",
    "pooledlabelsShufTrain = [pooledlabels[i] for i in PosElementsTrain]\n",
    "pooledfnatsShufTrain = [pooledfnats[i] for i in PosElementsTrain]\n",
    "\n",
    "pooledAGnameShufTest = [pooledAGname[i] for i in PosElementsTest]\n",
    "pooledIDsShufTest = [pooledIDs[i] for i in PosElementsTest]\n",
    "pooledantigenLatticeShufTest = [pooledantigenLattice[i] for i in PosElementsTest]\n",
    "pooledantibodyLatticeShufTest = [pooledantibodyLattice[i] for i in PosElementsTest]\n",
    "pooledlabelsShufTest = [pooledlabels[i] for i in PosElementsTest]\n",
    "pooledfnatsShufTest = [pooledfnats[i] for i in PosElementsTest]\n",
    "\n",
    "encodedLabelsTrain = np.array(list(map(binary, pooledlabelsShufTrain)))\n",
    "encodedLabelsTest = np.array(list(map(binary, pooledlabelsShufTest)))\n",
    "\n",
    "print(\"Train labels\", Counter(encodedLabelsTrain))\n",
    "print(\"Test labels\", Counter(encodedLabelsTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data enhancement: repeating the positive poses as to have as many positive and negative\n",
    "if balancingStrategy == 2:\n",
    "    # Now we look at the positions into the training only\n",
    "    #List positive elements in the training\n",
    "    listIDPos = [i for i in range(0,train_size) if encodedLabelsTrain[i] == 1]\n",
    "\n",
    "    #List negative elements in the training\n",
    "    listIDNeg = [i for i in range(0,train_size) if encodedLabelsTrain[i] == 0]\n",
    "\n",
    "    Npos = len(listIDPos)\n",
    "    Npos = len(listIDPos)\n",
    "    Nneg = len(listIDNeg)\n",
    "    NPosToAdd = Nneg - Npos\n",
    "    SelectedRepeatedPositions = random.choices(listIDPos, k=NPosToAdd)\n",
    "\n",
    "    # Now data enhancement\n",
    "    pooledAGnameShufTrain += [pooledAGnameShufTrain[i] for i in SelectedRepeatedPositions]\n",
    "    pooledIDsShufTrain += [pooledIDsShufTrain[i] for i in SelectedRepeatedPositions]\n",
    "    pooledantigenLatticeShufTrain += [pooledantigenLatticeShufTrain[i] for i in SelectedRepeatedPositions]\n",
    "    pooledantibodyLatticeShufTrain += [pooledantibodyLatticeShufTrain[i] for i in SelectedRepeatedPositions]\n",
    "    pooledlabelsShufTrain += [pooledlabelsShufTrain[i] for i in SelectedRepeatedPositions]\n",
    "    pooledfnatsShufTrain += [pooledfnatsShufTrain[i] for i in SelectedRepeatedPositions]\n",
    "\n",
    "    encodedLabelsTrain = np.array(list(map(binary, pooledlabelsShufTrain)))\n",
    "\n",
    "    print(\"The training data has been balanced (balancingStrategy == 2; not the test), new label distribution:\")\n",
    "    print(\"Train\", Counter(encodedLabelsTrain))\n",
    "    print(\"Test\", Counter(encodedLabelsTest))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the datasets are prepared, encoded labels are 0/1 but lattices are still as text. The generator function will create lattices as 3D tensors with AA or chem features\n",
    "# 1 one-hot, 3, shuffled, 11 = Chemical, 13 = Chemical shuffled\n",
    "if(condition == 3 or condition == 13):\n",
    "    random.shuffle(pooledlabelsShufTrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init\n",
      "Init\n"
     ]
    }
   ],
   "source": [
    "def myBalancedTrainGenerator():\n",
    "    N = len(pooledantigenLatticeShufTrain)\n",
    "\n",
    "    for k in range(0,nRotations):\n",
    "        randomizedIDs = [*range(0,N)]\n",
    "        random.shuffle(randomizedIDs)\n",
    "\n",
    "        print(\"Train data pack\", k, \"/\", nRotations, \" \", k*N, \"/\", nRotations * N, \" Processed\")\n",
    "\n",
    "        for i in range(0,N):\n",
    "\n",
    "            if(i % 10000 == 0):\n",
    "                print(\"+10000 done\")\n",
    "\n",
    "            antigenLat = pooledantigenLatticeShufTrain[randomizedIDs[i]]\n",
    "            antibodyLat = pooledantibodyLatticeShufTrain[randomizedIDs[i]]\n",
    "            label = encodedLabelsTrain[randomizedIDs[i]]\n",
    "\n",
    "            if((i + k) % nRotations != 0):  # one pose is not rotated, shift by 1 each block of data \n",
    "                [phi, theta] = getRandomRotation()  # Same rotation for antigen and antibody\n",
    "                hotEncodedAntigen = rotateLattice(textToTensor(antigenLat), [phi, theta])\n",
    "                hotEncodedAntibody = rotateLattice(textToTensor(antibodyLat), [phi, theta])\n",
    "            else: \n",
    "                hotEncodedAntigen = textToTensor(antigenLat)\n",
    "                hotEncodedAntibody = textToTensor(antibodyLat)\n",
    "\n",
    "            yield((hotEncodedAntigen, hotEncodedAntibody), label)\n",
    "\n",
    "class callableGenerator:\n",
    "    def __init__(self):\n",
    "        print(\"Init\")\n",
    "\n",
    "    def __call__(self):\n",
    "        return myBalancedTrainGenerator()\n",
    "\n",
    "trainingDataset = tf.data.Dataset.from_generator(callableGenerator(), \n",
    "    output_types=((tf.float32,tf.float32), tf.float32), \n",
    "    output_shapes=(([6,6,6,encodingDimension], [6,6,6,encodingDimension]), [])).batch(batch_size)            \n",
    "\n",
    "def myBalancedTestGenerator():\n",
    "    N = len(pooledantigenLatticeShufTest)\n",
    "\n",
    "    for k in range(0,nRotations):\n",
    "\n",
    "        randomizedIDs = [*range(0,N)]\n",
    "        random.shuffle(randomizedIDs)\n",
    "\n",
    "        print(\"Test data pack\", k, \"/\", nRotations, \" \", k*N, \"/\", nRotations * N, \" Processed\")\n",
    "\n",
    "        for i in range(0,N):\n",
    "\n",
    "            if(i % 10000 == 0):\n",
    "                print(\"+10000 done\")\n",
    "\n",
    "            antigenLat = pooledantigenLatticeShufTest[randomizedIDs[i]]\n",
    "            antibodyLat = pooledantibodyLatticeShufTest[randomizedIDs[i]]\n",
    "            label = encodedLabelsTest[randomizedIDs[i]]\n",
    "\n",
    "            if((i + k) % nRotations != 0):  # one pose is not rotated, shift by 1 each block of data \n",
    "                [phi, theta] = getRandomRotation()  # Same rotation for antigen and antibody\n",
    "                hotEncodedAntigen = rotateLattice(textToTensor(antigenLat), [phi, theta])\n",
    "                hotEncodedAntibody = rotateLattice(textToTensor(antibodyLat), [phi, theta])\n",
    "            else: \n",
    "                hotEncodedAntigen = textToTensor(antigenLat)\n",
    "                hotEncodedAntibody = textToTensor(antibodyLat)\n",
    "\n",
    "            yield((hotEncodedAntigen, hotEncodedAntibody), label)\n",
    "\n",
    "class callableGeneratorTest:\n",
    "    def __init__(self):\n",
    "        print(\"Init\")\n",
    "\n",
    "    def __call__(self):\n",
    "        return myBalancedTestGenerator()\n",
    "\n",
    "testDataset = tf.data.Dataset.from_generator(callableGeneratorTest(), \n",
    "    output_types=((tf.float32,tf.float32), tf.float32), \n",
    "    output_shapes=(([6,6,6,encodingDimension], [6,6,6,encodingDimension]), [])).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import average_precision_score - doesnt work for TF datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized CNN3D\n",
      "Initialized CNN3D\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "antibody (InputLayer)           [(None, 6, 6, 6, 20) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "antigen (InputLayer)            [(None, 6, 6, 6, 20) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cn_n3d_8 (CNN3D)                (None, 2, 2, 2, 128) 294880      antibody[0][0]                   \n",
      "                                                                 antigen[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 1024)         0           cn_n3d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 1024)         0           cn_n3d_8[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 2048)         0           flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 2048)         0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            2049        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 296,929\n",
      "Trainable params: 296,481\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    " # Instantiation of the ML architecture\n",
    "test = createDLABmodel()\n",
    "print(test.summary())\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss = 'binary_crossentropy'\n",
    "metrics = ['accuracy', 'FalseNegatives', 'FalsePositives', 'TrueNegatives', 'TruePositives', \n",
    "           'Precision', 'Recall', 'AUC'] #, average_precision_score] #, tf.compat.v1.metrics.average_precision_at_k]\n",
    "test.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train data pack 0 / 1   0 / 4000  Processed\n",
      "+10000 done\n",
      "      2/Unknown - 7s 3s/step - loss: 1.1909 - accuracy: 0.6685 - false_negatives: 341.0000 - false_positives: 985.0000 - true_negatives: 2565.0000 - true_positives: 109.0000 - precision: 0.0996 - recall: 0.2422 - auc: 0.4997Test data pack 0 / 1   0 / 1000  Processed\n",
      "+10000 done\n",
      "2/2 [==============================] - 9s 5s/step - loss: 1.1909 - accuracy: 0.6685 - false_negatives: 341.0000 - false_positives: 985.0000 - true_negatives: 2565.0000 - true_positives: 109.0000 - precision: 0.0996 - recall: 0.2422 - auc: 0.4997 - val_loss: 0.7194 - val_accuracy: 0.4630 - val_false_negatives: 30.0000 - val_false_positives: 507.0000 - val_true_negatives: 443.0000 - val_true_positives: 20.0000 - val_precision: 0.0380 - val_recall: 0.4000 - val_auc: 0.4379\n",
      "Epoch 2/5\n",
      "Train data pack 0 / 1   0 / 4000  Processed\n",
      "+10000 done\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.3900 - accuracy: 0.9038 - false_negatives: 106.0000 - false_positives: 279.0000 - true_negatives: 3271.0000 - true_positives: 344.0000 - precision: 0.5522 - recall: 0.7644 - auc: 0.8664Test data pack 0 / 1   0 / 1000  Processed\n",
      "+10000 done\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.3900 - accuracy: 0.9038 - false_negatives: 106.0000 - false_positives: 279.0000 - true_negatives: 3271.0000 - true_positives: 344.0000 - precision: 0.5522 - recall: 0.7644 - auc: 0.8664 - val_loss: 2.6788 - val_accuracy: 0.0500 - val_false_negatives: 0.0000e+00 - val_false_positives: 950.0000 - val_true_negatives: 0.0000e+00 - val_true_positives: 50.0000 - val_precision: 0.0500 - val_recall: 1.0000 - val_auc: 0.5569\n",
      "Epoch 3/5\n",
      "Train data pack 0 / 1   0 / 4000  Processed\n",
      "+10000 done\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.1309 - accuracy: 0.9498 - false_negatives: 164.0000 - false_positives: 37.0000 - true_negatives: 3513.0000 - true_positives: 286.0000 - precision: 0.8854 - recall: 0.6356 - auc: 0.9634Test data pack 0 / 1   0 / 1000  Processed\n",
      "+10000 done\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.1309 - accuracy: 0.9498 - false_negatives: 164.0000 - false_positives: 37.0000 - true_negatives: 3513.0000 - true_positives: 286.0000 - precision: 0.8854 - recall: 0.6356 - auc: 0.9634 - val_loss: 3.4681 - val_accuracy: 0.0500 - val_false_negatives: 0.0000e+00 - val_false_positives: 950.0000 - val_true_negatives: 0.0000e+00 - val_true_positives: 50.0000 - val_precision: 0.0500 - val_recall: 1.0000 - val_auc: 0.6360\n",
      "Epoch 4/5\n",
      "Train data pack 0 / 1   0 / 4000  Processed\n",
      "+10000 done\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0628 - accuracy: 0.9778 - false_negatives: 64.0000 - false_positives: 25.0000 - true_negatives: 3525.0000 - true_positives: 386.0000 - precision: 0.9392 - recall: 0.8578 - auc: 0.9917Test data pack 0 / 1   0 / 1000  Processed\n",
      "+10000 done\n",
      "2/2 [==============================] - 9s 4s/step - loss: 0.0628 - accuracy: 0.9778 - false_negatives: 64.0000 - false_positives: 25.0000 - true_negatives: 3525.0000 - true_positives: 386.0000 - precision: 0.9392 - recall: 0.8578 - auc: 0.9917 - val_loss: 2.5982 - val_accuracy: 0.0510 - val_false_negatives: 0.0000e+00 - val_false_positives: 949.0000 - val_true_negatives: 1.0000 - val_true_positives: 50.0000 - val_precision: 0.0501 - val_recall: 1.0000 - val_auc: 0.5268\n",
      "Epoch 5/5\n",
      "Train data pack 0 / 1   0 / 4000  Processed\n",
      "+10000 done\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9925 - false_negatives: 9.0000 - false_positives: 21.0000 - true_negatives: 3529.0000 - true_positives: 441.0000 - precision: 0.9545 - recall: 0.9800 - auc: 0.9991Test data pack 0 / 1   0 / 1000  Processed\n",
      "+10000 done\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0250 - accuracy: 0.9925 - false_negatives: 9.0000 - false_positives: 21.0000 - true_negatives: 3529.0000 - true_positives: 441.0000 - precision: 0.9545 - recall: 0.9800 - auc: 0.9991 - val_loss: 1.7922 - val_accuracy: 0.1330 - val_false_negatives: 6.0000 - val_false_positives: 861.0000 - val_true_negatives: 89.0000 - val_true_positives: 44.0000 - val_precision: 0.0486 - val_recall: 0.8800 - val_auc: 0.4517\n"
     ]
    }
   ],
   "source": [
    "#createError\n",
    "# Now, fit with or without compensated loss for imbalance (strategy 1), or without weights (0 or 2, in 2 the data is pre-balanced)\n",
    "if balancingStrategy == 1:\n",
    "    from sklearn.utils import class_weight\n",
    "    class_weights = class_weight.compute_class_weight('balanced',np.unique(trainLabels), trainLabels)\n",
    "    print(\"The loss function will be reweighted with weights\", str(class_weights))\n",
    "\n",
    "    history = test.fit(trainingDataset,validation_data=testDataset,\n",
    "                        class_weight=class_weights,\n",
    "                        epochs=nEpochs,\n",
    "                        verbose = 1)\n",
    "elif balancingStrategy != 1: # for option 2, it was already balanced as dataset processing\n",
    "    history = test.fit(trainingDataset,validation_data=testDataset, \n",
    "                        epochs=nEpochs,\n",
    "                        verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [1.1909161806106567, 0.3899695575237274, 0.1309470385313034, 0.06281618028879166, 0.025018878281116486], 'accuracy': [0.6685000061988831, 0.9037500023841858, 0.9497500061988831, 0.9777500033378601, 0.9925000071525574], 'false_negatives': [341.0, 106.0, 164.0, 64.0, 9.0], 'false_positives': [985.0, 279.0, 37.0, 25.0, 21.0], 'true_negatives': [2565.0, 3271.0, 3513.0, 3525.0, 3529.0], 'true_positives': [109.0, 344.0, 286.0, 386.0, 441.0], 'precision': [0.09963437169790268, 0.5521669387817383, 0.8854489326477051, 0.9391727447509766, 0.9545454382896423], 'recall': [0.24222221970558167, 0.7644444704055786, 0.6355555653572083, 0.8577777743339539, 0.9800000190734863], 'auc': [0.49970293045043945, 0.8663906455039978, 0.9633868932723999, 0.9916525483131409, 0.9991095662117004], 'val_loss': [0.7194469571113586, 2.6787657737731934, 3.4681050777435303, 2.5981674194335938, 1.7922313213348389], 'val_accuracy': [0.46299999952316284, 0.05000000074505806, 0.05000000074505806, 0.050999999046325684, 0.13300000131130219], 'val_false_negatives': [30.0, 0.0, 0.0, 0.0, 6.0], 'val_false_positives': [507.0, 950.0, 950.0, 949.0, 861.0], 'val_true_negatives': [443.0, 0.0, 0.0, 1.0, 89.0], 'val_true_positives': [20.0, 50.0, 50.0, 50.0, 44.0], 'val_precision': [0.03795066475868225, 0.05000000074505806, 0.05000000074505806, 0.050050050020217896, 0.04861878603696823], 'val_recall': [0.4000000059604645, 1.0, 1.0, 1.0, 0.8799999952316284], 'val_auc': [0.4379262924194336, 0.5568631887435913, 0.6359578967094421, 0.5268315672874451, 0.4516526460647583]}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Precision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-3a2539d5df2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#evaluation = test.evaluate(({'antibody': testKeysAntigen, 'antigen': testKeysAntibody}, [testLabels]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mf1hist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetF1TrainVal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# Writes the output in one line with all information\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfile_object\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'HistoryDLAB.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-03e1d3fe3adc>\u001b[0m in \u001b[0;36mgetF1TrainVal\u001b[1;34m(histDict)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetF1TrainVal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistDict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m     \u001b[0mtrainF1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Precision'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Recall'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m     \u001b[0mvalF1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_Precision'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_Recall'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrainF1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalF1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-03e1d3fe3adc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetF1TrainVal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistDict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m     \u001b[0mtrainF1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Precision'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Recall'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m     \u001b[0mvalF1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_Precision'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_Recall'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrainF1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalF1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Precision'"
     ]
    }
   ],
   "source": [
    "#evaluation = test.evaluate(({'antibody': testKeysAntigen, 'antigen': testKeysAntibody}, [testLabels]))\n",
    "print(history.history)\n",
    "f1hist = getF1TrainVal(history.history)\n",
    "# Writes the output in one line with all information\n",
    "file_object = open('HistoryDLABTrainVal.txt', 'a')\n",
    "file_object.write(\"successedAntigens\\tnSuccessAg\\tstopDataLeakage\\t\" + \"size_lattice\\t\" + \"balancingStrategy\\t\" + \"nEpochs\\t\" + \"nAntigens\\t\" + \"nPairsTot\\t\" + \"strategyNegatives\\t\" + \"repeat\\t\" + \"nRepeats\" + \"\\t\" + \"condition\\t\" + \"useAAchem\\t\" + \"nRotations\\t\" + \"groupSize\\t\" + \"fnatBind\\t\" + \"fnatDLABNegative\\t\" + \"batch_size\" + \"\\t\" + \"test.evaluate(([testKeysAntigen,testKeysAntibody], [testLabels]))\" + \"\\n\")\n",
    "file_object.write(str(successedAntigens) + \"\\t\" + str(len(successedAntigens)) + \"\\t\" + str(stopDataLeakage) + \n",
    "                  \"\\t\" + str(size_lattice) + \"\\t\" + str(balancingStrategy) + \"\\t\" + str(nEpochs) + \"\\t\" + str(nAntigens) + \n",
    "                  \"\\t\" + str(nPairsTot) + \"\\t\" + str(saveStrategyNegatives) + \"\\t\" + str(repeatLoop) + \"\\t\" + str(nRepeats)+\n",
    "                  \"\\t\" + str(condition) + \"\\t\" + str(useAAchem) + \"\\t\" + str(nRotations) + \"\\t\" + str(groupSize) + \n",
    "                  \"\\t\" + str(fnatBind) + \"\\t\" + str(fnatDLABNegative) + '\\t' + str(batch_size) + \n",
    "                  '\\t' + str(printMetrics(history.history, nEpochs)) + \"\\t\" + str(selectedAGnames) + \"\\t\" + str(f1hist) + \n",
    "                  \"\\t\" + str(history.history['AUC']) + \"\\t\" + str(history.history['val_AUC']) +               \n",
    "                  \"\\t\" + str(Counter(pooledlabelsAll[0:train_size])), str(Counter(pooledlabelsAll[train_size + groupSize: train_size + test_size + groupSize])) + \"\\n\")\n",
    "file_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PossibleDataID represents all positions that can be taken in the train/val. We have already taken\n",
    "\n",
    "#Elements from the test set that we reuse in the external\n",
    "PosInTestCanBeRetaken = [i for i in possibleDataIDs[train_size + groupSize: train_size + test_size + groupSize] if pooledIsInExt[i] == True]\n",
    "\n",
    "# Now, the instances that could have belonged in train/val but not taken\n",
    "PosInSharedClassesNotTakenTrainTest = [i for i in possibleDataIDs[train_size + test_size + groupSize:len(possibleDataIDs)] if (pooledIsInExt[i] == True and pooledIsInTrain[i] == True)]\n",
    "\n",
    "# Now, take all other positions that are in Ext but not in train (basically the positions that are inot in possibleDataIDs)\n",
    "# Note, this is not shuffled yet\n",
    "PosInExtButNotTrainTest = [i for i in range(len(pooledAGname)) if (pooledIsInExt[i] == True and pooledIsInTrain[i] == False)] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels in training+Ext: Counter({'I': 150, 'P': 50})\n",
      "Labels in training+Ext: Counter({0: 150, 1: 50})\n",
      "Labels not yet taken by train+val: Counter({'P': 17350, 'I': 17150})\n",
      "Labels not yet taken by train+val: Counter({1: 17350, 0: 17150})\n",
      "Labels available in Ext only: Counter({'L': 87850, 'N': 85850})\n",
      "Labels available in Ext only: Counter({0: 173700})\n"
     ]
    }
   ],
   "source": [
    "# This is only for printing\n",
    "pooledAGnameShufTestAndExt = [pooledAGname[i] for i in PosInTestCanBeRetaken]\n",
    "pooledIDsShufTestAndExt = [pooledIDs[i] for i in PosInTestCanBeRetaken]\n",
    "pooledantigenLatticeShufTestAndExt = [pooledantigenLattice[i] for i in PosInTestCanBeRetaken]\n",
    "pooledantibodyLatticeShufTestAndExt = [pooledantibodyLattice[i] for i in PosInTestCanBeRetaken]\n",
    "pooledlabelsShufTestAndExt = [pooledlabels[i] for i in PosInTestCanBeRetaken]\n",
    "pooledfnatsShufTestAndExt = [pooledfnats[i] for i in PosInTestCanBeRetaken]\n",
    "encodedLabelsTestAndExt = np.array(list(map(binary, pooledlabelsShufTestAndExt)))\n",
    "print(\"Labels in training+Ext:\", Counter(pooledlabelsShufTestAndExt))\n",
    "print(\"Labels in training+Ext:\", Counter(encodedLabelsTestAndExt))\n",
    "\n",
    "pooledAGnameShufTestAndExtAdd = [pooledAGname[i] for i in PosInSharedClassesNotTakenTrainTest]\n",
    "pooledIDsShufTestAndExtAdd = [pooledIDs[i] for i in PosInSharedClassesNotTakenTrainTest]\n",
    "pooledantigenLatticeShufTestAndExtAdd = [pooledantigenLattice[i] for i in PosInSharedClassesNotTakenTrainTest]\n",
    "pooledantibodyLatticeShufTestAndExtAdd = [pooledantibodyLattice[i] for i in PosInSharedClassesNotTakenTrainTest]\n",
    "pooledlabelsShufTestAndExtAdd = [pooledlabels[i] for i in PosInSharedClassesNotTakenTrainTest]\n",
    "pooledfnatsShufTestAndExtAdd = [pooledfnats[i] for i in PosInSharedClassesNotTakenTrainTest]\n",
    "encodedLabelsTestAndExtAdd = np.array(list(map(binary, pooledlabelsShufTestAndExtAdd)))\n",
    "print(\"Labels not yet taken by train+val:\", Counter(pooledlabelsShufTestAndExtAdd))\n",
    "print(\"Labels not yet taken by train+val:\", Counter(encodedLabelsTestAndExtAdd))\n",
    "# Now, only the classes that are new (not in the train)\n",
    "\n",
    "# Classes that were not in the train/eval but are desired in the external test\n",
    "# We will first take ALL data that has been ignored, then we are going to shuffle by blocks (IDs) \n",
    "# Then we just take a 'fair' amount.\n",
    "# Anyways, the external is going to be quantified separately on each label separately, so one can recalculate metrics later.\n",
    "\n",
    "pooledAGnameShufExtOnly = [pooledAGname[i] for i in PosInExtButNotTrainTest]\n",
    "pooledIDsShufExtOnly = [pooledIDs[i] for i in PosInExtButNotTrainTest]\n",
    "pooledantigenLatticeShufExtOnly = [pooledantigenLattice[i] for i in PosInExtButNotTrainTest]\n",
    "pooledantibodyLatticeShufExtOnly = [pooledantibodyLattice[i] for i in PosInExtButNotTrainTest]\n",
    "pooledlabelsShufExtOnly = [pooledlabels[i] for i in PosInExtButNotTrainTest]\n",
    "pooledfnatsShufExtOnly = [pooledfnats[i] for i in PosInExtButNotTrainTest]\n",
    "encodedLabelsExtOnly = np.array(list(map(binary, pooledlabelsShufExtOnly)))\n",
    "print(\"Labels available in Ext only:\", Counter(pooledlabelsShufExtOnly))\n",
    "print(\"Labels available in Ext only:\", Counter(encodedLabelsExtOnly))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling all instances possible for the ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels in Ext: Counter({'L': 87850, 'N': 85850, 'P': 17400, 'I': 17300})\n",
      "Labels in Ext: Counter({0: 191000, 1: 17400})\n",
      "Labels in Ext after shuffling: Counter({'L': 87850, 'N': 85850, 'P': 17400, 'I': 17300})\n",
      "Labels in Ext after shuffling: Counter({0: 191000, 1: 17400})\n"
     ]
    }
   ],
   "source": [
    "PosForExternal = PosInTestCanBeRetaken + PosInSharedClassesNotTakenTrainTest + PosInExtButNotTrainTest \n",
    "\n",
    "pooledAGnameExt = [pooledAGname[i] for i in PosForExternal]\n",
    "pooledIDsExt = [pooledIDs[i] for i in PosForExternal]\n",
    "pooledantigenLatticeExt = [pooledantigenLattice[i] for i in PosForExternal]\n",
    "pooledantibodyLatticeExt = [pooledantibodyLattice[i] for i in PosForExternal]\n",
    "pooledlabelsExt = [pooledlabels[i] for i in PosForExternal]\n",
    "pooledfnatsExt = [pooledfnats[i] for i in PosForExternal]\n",
    "encodedLabelsExt = np.array(list(map(binary, pooledlabelsExt)))\n",
    "print(\"Labels in Ext:\", Counter(pooledlabelsExt))\n",
    "print(\"Labels in Ext:\", Counter(encodedLabelsExt))\n",
    "\n",
    "shuffledPositionsPerGroup = shuffleListPositionsPerID(pooledIDsExt)\n",
    "\n",
    "pooledAGnameExt = [pooledAGnameExt[i] for i in shuffledPositionsPerGroup]\n",
    "pooledIDsExt = [pooledIDsExt[i] for i in shuffledPositionsPerGroup]\n",
    "pooledantigenLatticeExt = [pooledantigenLatticeExt[i] for i in shuffledPositionsPerGroup]\n",
    "pooledantibodyLatticeExt = [pooledantibodyLatticeExt[i] for i in shuffledPositionsPerGroup]\n",
    "pooledlabelsExt = [pooledlabelsExt[i] for i in shuffledPositionsPerGroup]\n",
    "pooledfnatsExt = [pooledfnatsExt[i] for i in shuffledPositionsPerGroup]\n",
    "encodedLabelsExt = np.array(list(map(binary, pooledlabelsExt)))\n",
    "print(\"Labels in Ext after shuffling:\", Counter(pooledlabelsExt))\n",
    "print(\"Labels in Ext after shuffling:\", Counter(encodedLabelsExt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are going to pick the same amount of each class\n",
    "nAvailable = len(pooledAGnameExt)\n",
    "listIDsO = [i for i in range(0, nAvailable) if pooledlabelsExt[i] == 'O']\n",
    "listIDsF = [i for i in range(0, nAvailable) if pooledlabelsExt[i] == 'F']\n",
    "listIDsI = [i for i in range(0, nAvailable) if pooledlabelsExt[i] == 'I']\n",
    "listIDsN = [i for i in range(0, nAvailable) if pooledlabelsExt[i] == 'N']\n",
    "listIDsL = [i for i in range(0, nAvailable) if pooledlabelsExt[i] == 'L']\n",
    "listIDsP = [i for i in range(0, nAvailable) if pooledlabelsExt[i] == 'P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just pick test_size of each class\n",
    "nPerClass = test_size\n",
    "balancedPositionsExt = []\n",
    "if(len(listIDsO)>0):\n",
    "    balancedPositionsExt = balancedPositionsExt + list(np.random.choice(listIDsO, size=nPerClass, replace=True))\n",
    "if(len(listIDsF)>0):\n",
    "    balancedPositionsExt = balancedPositionsExt + list(np.random.choice(listIDsF, size=nPerClass, replace=True))\n",
    "if(len(listIDsI)>0):\n",
    "    balancedPositionsExt = balancedPositionsExt + list(np.random.choice(listIDsI, size=nPerClass, replace=True))\n",
    "if(len(listIDsN)>0):\n",
    "    balancedPositionsExt = balancedPositionsExt + list(np.random.choice(listIDsN, size=nPerClass, replace=True))\n",
    "if(len(listIDsL)>0):\n",
    "    balancedPositionsExt = balancedPositionsExt + list(np.random.choice(listIDsL, size=nPerClass, replace=True))\n",
    "if(len(listIDsP)>0):\n",
    "    balancedPositionsExt = balancedPositionsExt + list(np.random.choice(listIDsP, size=nPerClass, replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels in Ext after shuffling: Counter({'I': 1000, 'N': 1000, 'L': 1000, 'P': 1000})\n",
      "Labels in Ext after shuffling: Counter({0: 3000, 1: 1000})\n"
     ]
    }
   ],
   "source": [
    "pooledAGnameExtBalanced = [pooledAGnameExt[i] for i in balancedPositionsExt]\n",
    "pooledIDsExtBalanced = [pooledIDsExt[i] for i in balancedPositionsExt]\n",
    "pooledantigenLatticeExtBalanced = [pooledantigenLatticeExt[i] for i in balancedPositionsExt]\n",
    "pooledantibodyLatticeExtBalanced = [pooledantibodyLatticeExt[i] for i in balancedPositionsExt]\n",
    "pooledlabelsExtBalanced = [pooledlabelsExt[i] for i in balancedPositionsExt]\n",
    "pooledfnatsExtBalanced = [pooledfnatsExt[i] for i in balancedPositionsExt]\n",
    "encodedLabelsExtBalanced = np.array(list(map(binary, pooledlabelsExtBalanced)))\n",
    "print(\"Labels in Ext after shuffling:\", Counter(pooledlabelsExtBalanced))\n",
    "print(\"Labels in Ext after shuffling:\", Counter(encodedLabelsExtBalanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now testing on the balanced class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init\n"
     ]
    }
   ],
   "source": [
    "def ExtGenerator():\n",
    "    N = len(pooledantigenLatticeExtBalanced)\n",
    "\n",
    "    for k in range(0,nRotations):\n",
    "\n",
    "        randomizedIDs = [*range(0,N)]\n",
    "        random.shuffle(randomizedIDs)\n",
    "\n",
    "        print(\"External data pack\", k, \"/\", nRotations, \" \", k*N, \"/\", nRotations * N, \" Processed\")\n",
    "\n",
    "        for i in range(0,N):\n",
    "\n",
    "            if(i % 10000 == 0):\n",
    "                print(\"+10000 done\")\n",
    "\n",
    "            antigenLat = pooledantigenLatticeExtBalanced[randomizedIDs[i]]\n",
    "            antibodyLat = pooledantibodyLatticeExtBalanced[randomizedIDs[i]]\n",
    "            label = encodedLabelsExtBalanced[randomizedIDs[i]]\n",
    "\n",
    "            if((i + k) % nRotations != 0):  # one pose is not rotated, shift by 1 each block of data \n",
    "                [phi, theta] = getRandomRotation()  # Same rotation for antigen and antibody\n",
    "                hotEncodedAntigen = rotateLattice(textToTensor(antigenLat), [phi, theta])\n",
    "                hotEncodedAntibody = rotateLattice(textToTensor(antibodyLat), [phi, theta])\n",
    "            else: \n",
    "                hotEncodedAntigen = textToTensor(antigenLat)\n",
    "                hotEncodedAntibody = textToTensor(antibodyLat)\n",
    "\n",
    "            yield((hotEncodedAntigen, hotEncodedAntibody), label)\n",
    "\n",
    "class callableGeneratorExt:\n",
    "    def __init__(self):\n",
    "        print(\"Init\")\n",
    "\n",
    "    def __call__(self):\n",
    "        return ExtGenerator()\n",
    "\n",
    "extDataset = tf.data.Dataset.from_generator(callableGeneratorExt(), \n",
    "    output_types=((tf.float32,tf.float32), tf.float32), \n",
    "    output_shapes=(([6,6,6,encodingDimension], [6,6,6,encodingDimension]), [])).batch(batch_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External data pack 0 / 1   0 / 4000  Processed\n",
      "+10000 done\n",
      "2/2 [==============================] - 4s 2s/step - loss: 1.3905 - accuracy: 0.3047 - false_negatives: 49.0000 - false_positives: 2732.0000 - true_negatives: 268.0000 - true_positives: 951.0000 - precision: 0.2582 - recall: 0.9510 - auc: 0.6877\n"
     ]
    }
   ],
   "source": [
    "history2 = test.evaluate(extDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object = open('HistoryDLABExt.txt', 'a')\n",
    "file_object.write(\"successedAntigens\\tnSuccessAg\\tstopDataLeakage\\t\" + \"size_lattice\\t\" + \"balancingStrategy\\t\" + \"nEpochs\\t\" + \"nAntigens\\t\" + \"nPairsTot\\t\" + \"strategyNegatives\\t\" + \"repeat\\t\" + \"nRepeats\" + \"\\t\" + \"condition\\t\" + \"useAAchem\\t\" + \"nRotations\\t\" + \"groupSize\\t\" + \"fnatBind\\t\" + \"fnatDLABNegative\\t\" + \"batch_size\" + \"\\t\" + \"metrics\" + \"\\n\")\n",
    "file_object.write(\"Ext\" + \"\\t\" + str(successedAntigens) + \"\\t\" + str(len(successedAntigens)) + \"\\t\" + str(stopDataLeakage) + \n",
    "                  \"\\t\" + str(size_lattice) + \"\\t\" + str(balancingStrategy) + \"\\t\" + str(nEpochs) + \"\\t\" + str(nAntigens) + \n",
    "                  \"\\t\" + str(nPairsTot) + \"\\t\" + str(saveStrategyNegatives) + \"\\t\" + str(repeatLoop) + \"\\t\" + str(nRepeats)+\n",
    "                  \"\\t\" + str(condition) + \"\\t\" + str(useAAchem) + \"\\t\" + str(nRotations) + \"\\t\" + str(groupSize) + \n",
    "                  \"\\t\" + str(fnatBind) + \"\\t\" + str(fnatDLABNegative) + '\\t' + str(batch_size) + \n",
    "                  '\\t' + str(history2) + \"\\t\" + str(selectedAGnames) +              \n",
    "                  \"\\t\" + str(Counter(pooledlabelsExtBalanced)) + \"\\n\")\n",
    "file_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3904939889907837,\n",
       " 0.304749995470047,\n",
       " 49.0,\n",
       " 2732.0,\n",
       " 268.0,\n",
       " 951.0,\n",
       " 0.2582134008407593,\n",
       " 0.9509999752044678,\n",
       " 0.6876912117004395]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
